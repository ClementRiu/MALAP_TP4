{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# <p style=\"text-align: center;\"> MALAP TP4 - Perceptron, SVMs </p>\n",
    "#### <p style=\"text-align: center;\"> Clément RIU et Anne SPITZ </p>\n",
    "#### <p style=\"text-align: center;\"> 13/04/2017 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fichier tools.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_data(data, labels=None):\n",
    "    \"\"\"\n",
    "    Affiche des donnees 2D\n",
    "    :param data: matrice des donnees 2d\n",
    "    :param labels: vecteur des labels (discrets)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cols, marks = [\"red\", \"blue\", \"green\", \"orange\", \"black\", \"cyan\"], [\".\", \"+\", \"*\", \"o\", \"x\", \"^\"]\n",
    "    if labels is None:\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker=\"x\")\n",
    "        return\n",
    "    for i, l in enumerate(sorted(list(set(labels.flatten())))):\n",
    "        plt.scatter(data[labels == l, 0], data[labels == l, 1], c=cols[i], marker=marks[i])\n",
    "\n",
    "        \n",
    "def plot_frontiere(data,f,step=20):\n",
    "    \"\"\" Trace un graphe de la frontiere de decision de f\n",
    "    :param data: donnees\n",
    "    :param f: fonction de decision\n",
    "    :param step: pas de la grille\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    grid,x,y=make_grid(data=data,step=step)\n",
    "    plt.contourf(x,y,f(grid).reshape(x.shape),256)\n",
    "\n",
    "\n",
    "def make_grid(data=None, xmin=-5, xmax=5, ymin=-5, ymax=5, step=20):\n",
    "    \"\"\" Cree une grille sous forme de matrice 2d de la liste des points\n",
    "    :param data: pour calcluler les bornes du graphe\n",
    "    :param xmin: si pas data, alors bornes du graphe\n",
    "    :param xmax:\n",
    "    :param ymin:\n",
    "    :param ymax:\n",
    "    :param step: pas de la grille\n",
    "    :return: une matrice 2d contenant les points de la grille\n",
    "    \"\"\"\n",
    "    if data is not None:\n",
    "        xmax, xmin, ymax, ymin = np.max(data[:, 0]), np.min(data[:, 0]), np.max(data[:, 1]), np.min(data[:, 1])\n",
    "    x, y = np.meshgrid(np.arange(xmin, xmax, (xmax - xmin) * 1. / step), np.arange(ymin, ymax, (ymax - ymin) * 1. / step))\n",
    "    grid = np.c_[x.ravel(), y.ravel()]\n",
    "    return grid, x, y\n",
    "\n",
    "\n",
    "def gen_arti(centerx=1, centery=1, sigma=0.1, nbex=1000, data_type=0, epsilon=0.02):\n",
    "    \"\"\" Generateur de donnees,\n",
    "        :param centerx: centre des gaussiennes\n",
    "        :param centery:\n",
    "        :param sigma: des gaussiennes\n",
    "        :param nbex: nombre d'exemples\n",
    "        :param data_type: 0: melange 2 gaussiennes, 1: melange 4 gaussiennes, 2:echequier\n",
    "        :param epsilon: bruit dans les donnees\n",
    "        :return: data matrice 2d des donnnes,y etiquette des donnnees\n",
    "    \"\"\"\n",
    "    if data_type == 0:\n",
    "        # melange de 2 gaussiennes\n",
    "        xpos = np.random.multivariate_normal([centerx, centerx], np.diag([sigma, sigma]), int(nbex // 2))\n",
    "        xneg = np.random.multivariate_normal([-centerx, -centerx], np.diag([sigma, sigma]), int(nbex // 2))\n",
    "        data = np.vstack((xpos, xneg))\n",
    "        y = np.hstack((np.ones(nbex // 2), -np.ones(nbex // 2)))\n",
    "    if data_type == 1:\n",
    "        # melange de 4 gaussiennes\n",
    "        xpos = np.vstack((np.random.multivariate_normal([centerx, centerx], np.diag([sigma, sigma]), int(nbex // 4)),\n",
    "                          np.random.multivariate_normal([-centerx, -centerx], np.diag([sigma, sigma]), int(nbex / 4))))\n",
    "        xneg = np.vstack((np.random.multivariate_normal([-centerx, centerx], np.diag([sigma, sigma]), int(nbex // 4)),\n",
    "                          np.random.multivariate_normal([centerx, -centerx], np.diag([sigma, sigma]), int(nbex / 4))))\n",
    "        data = np.vstack((xpos, xneg))\n",
    "        y = np.hstack((np.ones(nbex // 2), -np.ones(int(nbex // 2))))\n",
    "\n",
    "    if data_type == 2:\n",
    "        # echiquier\n",
    "        data = np.reshape(np.random.uniform(-4, 4, 2 * nbex), (nbex, 2))\n",
    "        y = np.ceil(data[:, 0]) + np.ceil(data[:, 1])\n",
    "        y = 2 * (y % 2) - 1\n",
    "    # un peu de bruit\n",
    "    data[:, 0] += np.random.normal(0, epsilon, nbex)\n",
    "    data[:, 1] += np.random.normal(0, epsilon, nbex)\n",
    "    # on mélange les données\n",
    "    idx = np.random.permutation((range(y.size)))\n",
    "    data = data[idx, :]\n",
    "    y = y[idx]\n",
    "    return data, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fichier tp4-etu.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, max_iter=100, eps=1e-3, projection=None, grad_alg=None):\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.grad_alg = grad_alg or self.grad_descent\n",
    "        self.projection = projection or (lambda x: x)  # projection fonction identite par default\n",
    "\n",
    "    def fit(self, data, y):\n",
    "        data = self.projection(data)\n",
    "        self.w = np.random.random((1, data.shape[1]))\n",
    "        self.histo_w = np.zeros((self.max_iter, data.shape[1]))\n",
    "        self.histo_f = np.zeros((self.max_iter, 1))\n",
    "        ylab = set(y.flat)\n",
    "        if len(ylab) != 2:\n",
    "            print(\"pas bon nombres de labels (%d)\" % (ylab,))\n",
    "            return\n",
    "        self.labels = {-1: min(ylab), 1: max(ylab)}\n",
    "        y = 2 * (y != self.labels[-1]) - 1\n",
    "        i = 0\n",
    "\n",
    "        self.grad_alg(data, y)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        data = self.projection(data)\n",
    "        return np.array([self.labels[x] for x in np.sign(data.dot(self.w.T)).flat]).reshape((len(data),))\n",
    "\n",
    "    def score(self, data, y):\n",
    "        return np.mean(self.predict(data) == y)\n",
    "\n",
    "    def get_eps(self):\n",
    "        return self.eps\n",
    "\n",
    "    def loss(self, data, y):\n",
    "        return hinge(self.w, data, y)\n",
    "\n",
    "    def grad_descent(self, data, y):\n",
    "        i = 0\n",
    "        while i < self.max_iter:\n",
    "            idx = range(len(data))\n",
    "            for j in idx:\n",
    "                self.w = self.w - self.get_eps() * self.loss_g(data[j], y[j:(j + 1)])\n",
    "            self.histo_w[i] = self.w\n",
    "            self.histo_f[i] = self.loss(data, y)\n",
    "            if i % 20 == 0: print(i, self.histo_f[i])\n",
    "            i += 1\n",
    "    \n",
    "    def loss_g(self, data, y):\n",
    "        return grad_hinge(self.w, data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fichier tp3-etu.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_usps(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        f.readline()\n",
    "        data = [[float(x) for x in l.split()] for l in f if len(l.split()) > 2]\n",
    "    tmp = np.array(data)\n",
    "    return tmp[:, 1:], tmp[:, 0].astype(int)\n",
    "\n",
    "\n",
    "def get_usps(l, datax, datay):\n",
    "    \"\"\" l : liste des chiffres a extraire\"\"\"\n",
    "    if type(l) != list:\n",
    "        resx = datax[datay == l, :]\n",
    "        resy = datay[datay == l]\n",
    "        return resx, resy\n",
    "    tmp = list(zip(*[get_usps(i, datax, datay) for i in l]))\n",
    "    tmpx, tmpy = np.vstack(tmp[0]), np.hstack(tmp[1])\n",
    "    idx = np.random.permutation(range(len(tmpy)))\n",
    "    return tmpx[idx, :], tmpy[idx]\n",
    "\n",
    "\n",
    "def show_usps(data):\n",
    "    plt.imshow(data.reshape((16, 16)), interpolation=\"nearest\", cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <p style=\"text-align: center;\"> 1 Implémentation du perceptron </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 1.1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def hinge(w, data, y):\n",
    "    \"\"\"\"\"\"\n",
    "    data, y, w = data.reshape(len(y), -1), y.reshape(-1, 1), w.reshape(1, -1)\n",
    "    return np.mean(np.maximum(0, -np.multiply(y, np.dot(data, w.T))))\n",
    "    \n",
    "def grad_hinge(w, data, y):\n",
    "    \"\"\"\"\"\"\n",
    "    data, y, w = data.reshape(len(y), -1), y.reshape(-1, 1), w.reshape(1, -1)\n",
    "    \n",
    "    value = - np.multiply(y, data)    \n",
    "    sign = np.multiply(y, np.dot(data, w.T)) < 0\n",
    "    \n",
    "    return np.mean(np.multiply(sign, value), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Test des fonctions hinge et grad_hinge\n",
    "w = np.random.random((3,))\n",
    "data = np.random.random((100, 3))\n",
    "y = np.random.randint(0, 2, size=(100, 1)) * 2 - 1\n",
    "\n",
    "print(hinge(w, data, y), hinge(w, data[0], y[0]), hinge(w, data[0, :], y[0]))\n",
    "print(grad_hinge(w, data, y), grad_hinge(w, data[0], y[0]).shape, grad_hinge(w, data[0, :], y[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 1.2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Generation de donnees\n",
    "xtrain, ytrain = gen_arti(data_type=0, epsilon=0.5)\n",
    "xtest, ytest = gen_arti(data_type=0, epsilon=0.5)\n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron(max_iter=100, eps=1e-2)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Le score de la classification est bon, et l'algorithme de descente de gradient converge bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 1.3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### Visualisation des poids\n",
    "w_tab = model.histo_w\n",
    "line = []\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plt.title(\"Évolution des poids en fonction de \")\n",
    "line.append(plt.plot(w_tab[:,0], label=\"w_1\"))\n",
    "line.append(plt.plot(w_tab[:,1], label=\"w_2\"))\n",
    "plt.xlabel(\"Itération\")\n",
    "plt.ylabel(\"Valeur\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handler_map={line[0][0]: HandlerLine2D(numpoints=4)})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "La solution n'est pas unique, le résultat de la minimisation de la fonction dépend des paramètres initiaux et de l'ordre dans lequel les points sont parcourus.\n",
    "\n",
    "Dans ce cas, la frontière est pertinente : elle sépare bien les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Generation de donnees\n",
    "xtrain, ytrain = gen_arti(data_type=0, epsilon=5.5)\n",
    "xtest, ytest = gen_arti(data_type=0, epsilon=5.5)\n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron(max_iter=100, eps=1e-2)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lorsque le modèle est trop bruité, ce modèle ne parvient plus à séparer, le résultat correspond alors presque à un prédicteur constant.\n",
    "\n",
    "### Question 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Perceptron_other_grad:\n",
    "    def __init__(self, max_iter=100, eps=1e-3, projection=None, grad_alg=\"grad_descent\", k=1):\n",
    "        fonc_dic = {\"grad_descent\":self.grad_descent, \"batch\": self.batch, \"stochastic\": self.stochastic}\n",
    "        \n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.k=k\n",
    "        \n",
    "        self.grad_alg = fonc_dic[grad_alg]\n",
    "        self.projection = projection or (lambda x: x)  # projection fonction identite par default\n",
    "\n",
    "    def fit(self, data, y):\n",
    "        data = self.projection(data)\n",
    "        self.w = np.random.random((1, data.shape[1]))\n",
    "        self.histo_w = np.zeros((self.max_iter, data.shape[1]))\n",
    "        self.histo_f = np.zeros((self.max_iter, 1))\n",
    "        ylab = set(y.flat)\n",
    "        if len(ylab) != 2:\n",
    "            print(\"pas bon nombres de labels (%d)\" % (ylab,))\n",
    "            return\n",
    "        self.labels = {-1: min(ylab), 1: max(ylab)}\n",
    "        y = 2 * (y != self.labels[-1]) - 1\n",
    "        i = 0\n",
    "\n",
    "        self.grad_alg(data, y)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        data = self.projection(data)\n",
    "        return np.array([self.labels[x] for x in np.sign(data.dot(self.w.T)).flat]).reshape((len(data),))\n",
    "\n",
    "    def score(self, data, y):\n",
    "        return np.mean(self.predict(data) == y)\n",
    "\n",
    "    def get_eps(self):\n",
    "        return self.eps\n",
    "\n",
    "    def loss(self, data, y):\n",
    "        return hinge(self.w, data, y)\n",
    "\n",
    "    def grad_descent(self, data, y):\n",
    "        i = 0\n",
    "        while i < self.max_iter:\n",
    "            idx = range(len(data))\n",
    "            for j in idx:\n",
    "                self.w = self.w - self.get_eps() * self.loss_g(data[j], y[j:(j + 1)])\n",
    "            self.histo_w[i] = self.w\n",
    "            self.histo_f[i] = self.loss(data, y)\n",
    "            if i % 20 == 0: print(i, self.histo_f[i])\n",
    "            i += 1\n",
    "            \n",
    "    def batch(self, data, y):\n",
    "        i = 0\n",
    "        while i < self.max_iter:\n",
    "            idx = range(len(data))\n",
    "            grad = 0\n",
    "            for j in idx:\n",
    "                grad+=self.loss_g(data[j], y[j:(j + 1)])\n",
    "            self.w = self.w - self.get_eps() * grad/len(data)\n",
    "            self.histo_w[i] = self.w\n",
    "            self.histo_f[i] = self.loss(data, y)\n",
    "            if i % 20 == 0: print(i, self.histo_f[i])\n",
    "            i += 1\n",
    "        \n",
    "    def stochastic(self, data, y):\n",
    "        i = 0\n",
    "        while i < self.max_iter:\n",
    "            idx = np.arange(len(data))\n",
    "            np.random.shuffle(idx)\n",
    "            for j in idx:\n",
    "                self.w = self.w - self.get_eps() * self.loss_g(data[j], y[j:(j + 1)])\n",
    "            self.histo_w[i] = self.w\n",
    "            self.histo_f[i] = self.loss(data, y)\n",
    "            if i % 20 == 0: print(i, self.histo_f[i])\n",
    "            i += 1\n",
    "    \n",
    "    def loss_g(self, data, y):\n",
    "        return grad_hinge(self.w, data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Generation de donnees\n",
    "eps = 0.5\n",
    "xtrain, ytrain = gen_arti(data_type=0, epsilon=eps)\n",
    "xtest, ytest = gen_arti(data_type=0, epsilon=eps)\n",
    "\n",
    "### Batch descent\n",
    "print(\"Batch, pour un epsilon de {} :\".format(eps))\n",
    "model = Perceptron_other_grad(max_iter=200, eps=1e-2, grad_alg=\"batch\")\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "\n",
    "### Descente stochastique\n",
    "print(\"\\nStochastic, pour un epsilon de {} :\".format(eps))\n",
    "model = Perceptron_other_grad(max_iter=100, eps=1e-2, grad_alg=\"stochastic\")\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "\n",
    "eps = 2\n",
    "xtrain, ytrain = gen_arti(data_type=0, epsilon=eps)\n",
    "xtest, ytest = gen_arti(data_type=0, epsilon=eps)\n",
    "\n",
    "\n",
    "### Batch descent\n",
    "print(\"\\nBatch, pour un epsilon de {} :\".format(eps))\n",
    "model = Perceptron_other_grad(max_iter=160, eps=1e-2, grad_alg=\"batch\")\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "\n",
    "### Descente stochastique\n",
    "print(\"\\nStochastic, pour un epsilon de {} :\".format(eps))\n",
    "model = Perceptron_other_grad(max_iter=160, eps=1e-2, grad_alg=\"stochastic\")\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "\n",
    "\n",
    "eps = 10\n",
    "xtrain, ytrain = gen_arti(data_type=0, epsilon=eps)\n",
    "xtest, ytest = gen_arti(data_type=0, epsilon=eps)\n",
    "\n",
    "\n",
    "### Batch descent\n",
    "print(\"\\nBatch, pour un epsilon de {} :\".format(eps))\n",
    "model = Perceptron_other_grad(max_iter=160, eps=1e-2, grad_alg=\"batch\")\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "\n",
    "### Descente stochastique\n",
    "print(\"\\nStochastic, pour un epsilon de {} :\".format(eps))\n",
    "model = Perceptron_other_grad(max_iter=160, eps=1e-2, grad_alg=\"stochastic\")\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparant la descente batch et la descente stochastique sur plusieurs datasets, on constate que la descente stochastique converge bien plus rapidement vers sa valeur optimale de l'erreur ; en revanche, pour des datasets  dispersés, la descente batch performe mieux que la descente stochastique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <p style=\"text-align: center;\"> 2 Données USPS </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2.1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Observation de données réelles :\n",
    "xuspstrain, yuspstrain = load_usps(\"USPS_train.txt\")\n",
    "xuspstest, yuspstest = load_usps(\"USPS_test.txt\")\n",
    "x16train, y16train = get_usps([1, 6], xuspstrain, yuspstrain)\n",
    "x28train, y28train = get_usps([2, 8], xuspstrain, yuspstrain)\n",
    "x16test, y16test = get_usps([1, 6], xuspstest, yuspstest)\n",
    "x28test, y28test = get_usps([2, 8], xuspstest, yuspstest)\n",
    "x1268train = np.concatenate((x16train, x28train))\n",
    "x1268test = np.concatenate((x16test, x28test))\n",
    "y1268train = np.concatenate((y16train, y28train))\n",
    "y1268test = np.concatenate((y16test, y28test))\n",
    "y1268train[y1268train == 2] = 1\n",
    "y1268train[y1268train == 8] = 6\n",
    "y1268test[y1268test == 2] = 1\n",
    "y1268test[y1268test == 8] = 6\n",
    "\n",
    "x_chiffre_train = x1268train\n",
    "x_chiffre_test = x1268test\n",
    "y_chiffre_train = y1268train\n",
    "y_chiffre_test = y1268test\n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron(eps=1e-2)\n",
    "model.fit(x_chiffre_train, y_chiffre_train)\n",
    "print(\"score en train : \", model.score(x_chiffre_train, y_chiffre_train))\n",
    "print(\"score en test : \", model.score(x_chiffre_test, y_chiffre_test))\n",
    "\n",
    "plt.figure(num=None, figsize=(20, 10))\n",
    "show_usps(np.reshape(model.w, (16, 16)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Le score est relativement élevé, le perceptron arrive bien à séparer les deux chiffres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2.2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "err_train = []\n",
    "err_test = []\n",
    "iter_range = range(10, 211, 40)\n",
    "\n",
    "for max_iter in iter_range:\n",
    "    ### Apprentissage\n",
    "    model = Perceptron(max_iter=max_iter, eps=1e-2)\n",
    "    model.fit(x_chiffre_train, y_chiffre_train)\n",
    "    err_train.append(model.score(x_chiffre_train, y_chiffre_train))\n",
    "    err_test.append(model.score(x_chiffre_test, y_chiffre_test))\n",
    "\n",
    "\n",
    "line = []\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plt.title(\"Évolution des erreurs en fonction du nombre d'itération maximale\")\n",
    "line.append(plt.plot(iter_range, err_train, label=\"train\"))\n",
    "line.append(plt.plot(iter_range, err_test, label=\"test\"))\n",
    "plt.xlabel(\"Nombre d'itération maximum\")\n",
    "plt.ylabel(\"Erreur\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handler_map={line[0][0]: HandlerLine2D(numpoints=4)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On observe du sur-apprentissage. En effet, l'erreur sur l'ensemble d'apprentissage croit plus vite que l'erreur sur l'ensemble de test qui presque stagne à partir de 50 itérations au maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain_tab = []\n",
    "ytrain_tab = []\n",
    "xtest_tab = []\n",
    "ytest_tab = []\n",
    "\n",
    "for i in range(5):\n",
    "    xtrain_loc, ytrain_loc = get_usps([i, i + 5], xuspstrain, yuspstrain)\n",
    "    xtest_loc, ytest_loc = get_usps([i, i + 5], xuspstest, yuspstest)\n",
    "    xtrain_tab.append(xtrain_loc)\n",
    "    ytrain_tab.append(ytrain_loc)\n",
    "    xtest_tab.append(xtest_loc)\n",
    "    ytest_tab.append(ytest_loc)\n",
    "    \n",
    "xtrain = xtrain_tab[0]\n",
    "ytrain = ytrain_tab[0]\n",
    "xtest = xtest_tab[0]\n",
    "ytest = ytest_tab[0]\n",
    "\n",
    "for i in range(4):\n",
    "    np.concatenate((xtrain, xtrain_tab[i+1]))\n",
    "    np.concatenate((ytrain, ytrain_tab[i+1]))\n",
    "    np.concatenate((xtest, xtest_tab[i+1]))\n",
    "    np.concatenate((ytest, ytest_tab[i+1]))\n",
    "\n",
    "ytrain[ytrain != 0] = 1\n",
    "ytest[ytest != 0] = 1\n",
    "### Apprentissage\n",
    "model = Perceptron(eps=1e-2)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le perceptron est donc performant même sur des classifications un contre tous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <p style=\"text-align: center;\"> 3 Expréssivité et feature map </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3.1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Generation de donnees\n",
    "\n",
    "xtrain, ytrain = gen_arti(data_type=1, epsilon=0.2)\n",
    "xtest, ytest = gen_arti(data_type=1, epsilon=0.2)\n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron(eps=1e-2)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()\n",
    "\n",
    "### Generation de donnees\n",
    "\n",
    "xtrain, ytrain = gen_arti(data_type=2, epsilon=0.2)\n",
    "xtest, ytest = gen_arti(data_type=2, epsilon=0.2)\n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron(eps=1e-2)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Le modèle ne peux pas séparer les données non-linéairement séparables. Cela est normal car il s'agit d'un modèle linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3.2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Dans l'espace de dimension 6, la frontière de décision sera un hyperplan de dimension 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def poly(x):\n",
    "    \"\"\"\"\"\"\n",
    "    return np.array([np.ones(x.shape[0]), x[:, 0], x[:, 1], x[:, 0] * x[:, 1], x[:, 0] ** 2, x[:, 1] ** 2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Generation de donnees\n",
    "\n",
    "xtrain, ytrain = gen_arti(data_type=1, epsilon=0.2)\n",
    "xtest, ytest = gen_arti(data_type=1, epsilon=0.2)\n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron(eps=1e-2, projection=poly)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3.3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Si $o^i$ est le $i^{ème}$ point choisit de l'espace, la $i^{ème}$ dimension correspond à la proximité avec ce point : plus la valeur est grande et plus le point est dans une région proche du point choisi, à l'inverse, une valeur faible signigie que le point en est loin.\n",
    "\n",
    "Les poids $w^i$ correspondent alors à l'importance pour classifier de la région aux alentours du points $o^i$. Un poids nul signifie que cette région de l'espace n'est pas pertinente pour classer, et un poids positif élevé signifie que les points proches de ce point sont des points de la classe $+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def k(x, x_p, sigma):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    return np.exp(- np.linalg.norm(x - x_p, axis=0) / 2 / sigma**2)\n",
    "\n",
    "def kernel(x, o, n_dim=6, sigma=0.5):\n",
    "    \"\"\"\"\"\"\n",
    "    n_ex, n_dim_in  = x.shape\n",
    "    \n",
    "    x_proj = np.zeros((n_ex, n_dim))\n",
    "\n",
    "    for dim in range(n_dim):\n",
    "        for example in range(n_ex):\n",
    "            x_proj[example, dim] = k(x[example, :], o[dim, :], sigma)\n",
    "    \n",
    "    return x_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Generation de donnees\n",
    "\n",
    "xtrain, ytrain = gen_arti(data_type=1, epsilon=0.2)\n",
    "xtest, ytest = gen_arti(data_type=1, epsilon=0.2)\n",
    "\n",
    "n_dim = 20\n",
    "gen=\"autre\"\n",
    "\n",
    "n_ex, n_dim_in  = xtrain.shape\n",
    "\n",
    "if gen == \"rand\":\n",
    "    o = np.random.rand(n_dim, n_dim_in)\n",
    "    projec = lambda x : kernel(x, type=\"rand\", n_dim=20)\n",
    "else :\n",
    "    index = np.random.randint(n_ex, size=n_dim)\n",
    "    o = xtrain[index, :]\n",
    "    projec = lambda x : kernel(x, o, n_dim=n_dim, sigma=3)\n",
    "    \n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron(eps=1e-2, projection=projec)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Le modèle arrive bien à séparer suite au prolongement. Néanmoins, il faut bien paramétrer le nombre de dimension pour obtenir des bons résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Perceptron_reg:\n",
    "    def __init__(self, max_iter=100, eps=1e-3, lmb=0.1, projection=None):\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.lmb = lmb\n",
    "        self.projection = projection or (lambda x: x)  # projection fonction identite par default\n",
    "\n",
    "    def fit(self, data, y):\n",
    "        data = self.projection(data)\n",
    "        self.w = np.random.random((1, data.shape[1]))\n",
    "        self.histo_w = np.zeros((self.max_iter, data.shape[1]))\n",
    "        self.histo_f = np.zeros((self.max_iter, 1))\n",
    "        ylab = set(y.flat)\n",
    "        if len(ylab) != 2:\n",
    "            print(\"pas bon nombres de labels (%d)\" % (ylab,))\n",
    "            return\n",
    "        self.labels = {-1: min(ylab), 1: max(ylab)}\n",
    "        y = 2 * (y != self.labels[-1]) - 1\n",
    "        i = 0\n",
    "        while i < self.max_iter:\n",
    "            idx = range(len(data))\n",
    "            for j in idx:\n",
    "                self.w = self.w - self.get_eps() * self.loss_g(data[j], y[j:(j + 1)])\n",
    "            self.histo_w[i] = self.w\n",
    "            self.histo_f[i] = self.loss(data, y)\n",
    "            if i % 20 == 0: print(i, self.histo_f[i])\n",
    "            i += 1\n",
    "\n",
    "    def predict(self, data):\n",
    "        data = self.projection(data)\n",
    "        return np.array([self.labels[x] for x in np.sign(data.dot(self.w.T)).flat]).reshape((len(data),))\n",
    "\n",
    "    def score(self, data, y):\n",
    "        return np.mean(self.predict(data) == y)\n",
    "\n",
    "    def get_eps(self):\n",
    "        return self.eps\n",
    "\n",
    "    def loss(self, data, y):\n",
    "        return hinge(self.w, data, y) + self.lmb * np.linalg.norm(self.w) ** 2\n",
    "\n",
    "    def loss_g(self, data, y):\n",
    "        return grad_hinge(self.w, data, y) + 2 * self.lmb * self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "La formulation choisie rajoute de la régularité pour éviter le sur-apprentissage. En effet, lorsque le modèle sur-apprend, les poids ont tendance à grandir pour créer des pics de Dirach autour des poinds d'apprentissage. Rajouter un terme proportionnel à la norme du vecteur poids contrebalance cet effet.\n",
    "\n",
    "Nous pouvons toujours utiliser la descente de gradient, en rajoutant le terme $2 w$ au gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Apprentissage\n",
    "model = Perceptron_reg(eps=1e-2, lmb=0.001, projection=projec)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()\n",
    "\n",
    "### Apprentissage\n",
    "model = Perceptron_reg(eps=1e-2, lmb=0.01, projection=projec)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"score en train : \", model.score(xtrain, ytrain))\n",
    "print(\"score en test : \", model.score(xtest, ytest))\n",
    "\n",
    "#### Tracer de frontiere\n",
    "plt.figure(num=None, figsize=(15, 15))\n",
    "plot_frontiere(xtrain, model.predict, 50)\n",
    "plot_data(xtrain, ytrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On peut voir que si $\\lambda$, le paramètre de régularisation, est trop élevé, la qualité du modèle décroit car le modèle se simplifie pour tendre vers un modèle constant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
